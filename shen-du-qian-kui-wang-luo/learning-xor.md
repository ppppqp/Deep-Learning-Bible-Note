# Learning XOR

XOR函数的逻辑是，当两个二进制值$$x_{1}$$和$$x_{2}$$恰好有一个为1时，返回1。这通过线性模型是无论如何无法做到的，因为线性模型保证了当只有一个变量变化时，输出和该变量保证是线性关系。然而，当$$x_{1}$$保持1和0时，输出和$$x_{2}$$的关系是递减和递增，所以不存在一个确定的$$x_{2}$$的系数，方程是无解的。

当我们使用深度前馈网络时，我们能够将原始的空间映射到一个能线性表示的空间，来进行拟合。比如，最开始的空间时

| $$x_{1}$$ | $$x_{2}$$ | output |
| :--- | :--- | :--- |
| 0 | 0 | 0 |
| 0 | 1 | 1 |
| 1 | 0 | 1 |
| 1 | 1 | 0 |

经过$$h{1} = x{1}+x{2}$$,$$h{2} = \max{(0,x{1}+x{2}-1)}$$的变换，就可以得到新的空间

| $$x_{1}$$ | $$x_{2}$$ | $$h_{1}$$ | $$h_{2}$$ | output |
| :--- | :--- | :--- | :--- | :--- |
| 0 | 0 | 0 | 0 | 0 |
| 0 | 1 | 1 | 0 | 1 |
| 1 | 0 | 1 | 0 | 1 |
| 1 | 1 | 2 | 1 | 0 |

由此可见，我们只需要$$output = h{1}-2h_{2}$$即可。

注意到，得到$$h_{2}$$的公式中，我们用到了$$\max{(0,)}$$,这个能去除模型的线性，帮助形成更复杂的函数。这个也叫ReLU激活函数。

