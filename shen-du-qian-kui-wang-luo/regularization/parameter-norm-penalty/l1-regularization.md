# L1 Regularization

L1正则化被定义为$$ \Omega(\theta) = ||w||_{1} = \sum{i} =|w_{i}|$$即各个参数的绝对值之和。 我们将探讨使用L1正则化和L2的区别。

同样，正则化的目标函数则表示为$$\widetilde J(w; X,y) = \alpha||w||_{1} + J(w; X,y)$$，梯度\(其实是次梯度，因为绝对值函数不可导\)表示为$$\nabla_{w}\widetilde J(w; X,y) = \alpha sign(w) + \nabla_{w} J(w; X,y)$$。注意到绝对值函数引入了一个符号函数。由此可见，L1的效果不是线性地缩放每一个$$w_i$$，**而是给他们每一个添加了一项符号相同的常数**$$\alpha$$**。**

> 简单来说就是，相比缩放每一项（见L2 Regularization章节）, L1 Regularization直接加了个常数

在这种情况下，就不一定能得到$$J(X,y;w)$$的二次近似的直接算数解（L2正则化的时候可以）。简单线性模型具有二次代价函数，我们可以通过泰勒级数表示。或者我们可以设想，这是逼近更复杂模型的代价函数的截断泰勒级数。在这个设定下，梯度是$$\nabla _{w} \widetilde J(w)=H(w-w^{*})$$

> 这段的理解是，如果目标函数是二次的，则泰勒展开最多二项，和原函数相同（二次以上的导数为零）。L2正则化项$$\dfrac{1}{2}\alpha w^{T}w$$是二次的，所以可以**完全泰勒展开**，然后通过二次函数的计算公式求极值。L1的正则化是$$\alpha ||w||_{1} $$是高于二次的，即有高于二阶导数的导数。只能通过**泰勒展开，二阶截断来近似成二阶函数。**

\*\*\*\*

一般形式的Hessian矩阵，我们无法得到直接清晰的代数表达式，因此我们进一步简化假设Hessian是对角的，即$$H = diag([H_{1, 1}, ...H_{n, n}])$$，其中每个$$H_{i,i} > 0$$。如果数据已经被预处理（比如PCA），去除了特征之间的相关性，则这一假设成立。

> 理解：hessian矩阵是标量-向量函数的二阶导数。一阶导数是gradient（梯度），是一个向量，表示自变量向量中每一项和因变量标量的关系。$$\nabla_w = (\dfrac{\delta y}{\delta w_1}, \dfrac{\delta y}{\delta w_2},...)=(y_{w_1}, y_{w_2},...)$$
>
> 二阶导数是hessian矩阵，是向量-向量的求导，类似多元微积分中的Jacobian，每一项表示自变量向量中每一项和因变量向量中的每一项的关系。$$H =\dfrac{\delta \nabla_w}{\delta w} =  \begin{bmatrix}\dfrac{\delta y_{w_1}}{\delta w_1} & \dfrac{\delta y_{w_1}}{\delta w_2} \\ \dfrac{\delta y_{w_2}}{\delta w_1} & \dfrac{\delta y_{w_2}}{\delta w_2}\\\end{bmatrix}$$
>
> 这边可以假设为对角矩阵，就是假设不同特征之间没有相关性，即不同特征之间求二阶导数为零（看作常量）。只有相同特征，才具有二阶相关性，也就是为什么非零值都分布在对角线。

我们可以将L1正则化目标函数的二次近似分解成关于参数的求和：$$\widetilde J(w; X,y) = J(w^*; X, y)+\sum_i[\dfrac{1}{2}H_{i,i}(w_i-w_i^*)^2+\alpha |w_i|]$$，对每一个维度i存在一个解析解，合起来可以最小化这个近似代价函数$$w_i = sign(w_i^*) \max\{|w_i^*| - \dfrac{\alpha}{H_{i,i}}, 0\}$$

> 正则化目标函数的二次近似分解成相关参数的求和：$$\widetilde J$$的二次近似是J的二次近似+L1正则项。上面的第一个式子就是将$$w^TH$$和$$|w_i|$$的写成element-wise的形式。最小化这个函数，本质上是个二次函数优化问题。对该函数求导得到$$ H_{i,j}(w_i - w_i^*) + \alpha sign(w_i) = 0$$, $$w_i = \dfrac{-\alpha sign(w_i)}{H_{i,i}}+w_i^* $$
>
> * 如果$$w_i^* > \dfrac{\alpha}{H_{i,i}}$$，则$$w_i = w_i^* - \dfrac{\alpha}{H_{i,i}} > 0$$， 符合条件
> * 如果$$\dfrac{\alpha}{H_{i,j}}> w_i^* > -\dfrac{\alpha}{H_{i,i}}$$，则$$w_i$$无论取什么值都不能符合条件，所以此时根据单调性只有$$w_i = 0$$
> * 如果$$ w_i^* < -\dfrac{\alpha}{H_{i,i}}$$则$$w_i = w_i^* + \dfrac{\alpha}{H_{i,i}} < 0$$
>
> 这里的分段讨论有点像高考数学...还挺复杂，原书一笔带过了

考虑以上三种情况。对于绝对值较大的$$w^*$$，正则化不会将$$w_i$$的最优值推至零，而仅仅会移动$$\dfrac{\alpha}{H_{i,i}}$$的距离。如果绝对值较小，则会直接推至零。相比L2，L1能产生更加稀疏的解，即最优值有很多是0。这种稀疏性可以被应用在特征选择机制上，来化简机器学习问题。

> L1是减小一个值，有可能减小到零。L2是乘以一个稀疏，保持非零性



