---
description: 提前终止
---

# Early Stopping

当训练有足够的表示能甚至会过拟合的大模型是，经常会观察到，训练误差随着时间推移逐渐降低但是验证集的误差会再次上升。这意味着我们只要返回验证集误差最低的参数，就可以获得最优的模型。

### 提前终止的理解

我们可以认为提前终止是非常高效的超参数选择方法：**选择训练步数，也是一种正则化方式**。在提前终止的情况下，我们控制拟合训练步数来控制模型的有效容量。大多数超参数的选择必须使用高代价的猜测和检查过程。我们需要在开始训练的时候猜测超参数然后运行几个步骤检查训练效果。训练时间是**唯一一个比较好尝试的超参数**。

## 提前终止的一些考虑

### 问题一：定期评估验证集 <a id="&#x95EE;&#x9898;&#x4E00;&#xFF1A;&#x5B9A;&#x671F;&#x8BC4;&#x4F30;&#x9A8C;&#x8BC1;&#x96C6;"></a>

> 通过使用CPU/GPU并行计算解决，或者使用比较小的验证集合和取比较小的验证频率

### 问题二：保存参数副本

> 一般可忽略，可以储存在硬盘中

### 问题三：充分利用训练数据 <a id="&#x95EE;&#x9898;&#x4E09;&#xFF1A;&#x5145;&#x5206;&#x5229;&#x7528;&#x8BAD;&#x7EC3;&#x6570;&#x636E;"></a>

提前终止需要验证集，这意味着某些训练数据不能被馈送到模型。为了更好地利用这一额外的数据，我们可以在完成提前终止的首次训练之后，进行额外的训练。在第二轮，即额外的训练步骤中，所有的训练数据都被包括在内。

> （1）记录提前终止时的步数，使用所有数据集，重新训练相同的步数。 
>
> （2）保持第一轮训练获得的参数，记录提前终止时的loss，使用所有数据集，继续训练，直至验证集的平均loss低于记录的loss。

## 提前终止的正则化效果

提前终止的正真效果是什么呢？提前终止可以将优化过程的参数空间限制在初始值的小邻域内。想象使用学习率$$\epsilon$$优化$$\tau $$个步骤，我们可以将$$\epsilon \tau$$作为有效容量的度量。假设梯度有限，限制迭代次数和学习速率就能够限制$$\theta_0$$到达的参数空间的大小。从这个意义上$$\epsilon \tau$$的效果就像是权重衰减系数的倒数。

> $$\epsilon \tau$$越小，探索的空间越小，正则化效果越厉害

### L2正则化对参数的影响

在最佳值$$w^*$$附近以二次近似建模代价函数$$\hat J(\theta) = J(w ^*) + \dfrac{1}{2}(w-w^*)^TH(w-w^*)$$其中$$H$$是$$w$$在$$w^*$$点的Hessian。在局部泰勒级数逼近下，梯度为$$\nabla_w \hat J(w) = H(w-w^*)$$

每一步更新可以表示为

$$\begin{aligned}w^{(\tau)} &= w^{(\tau -1)} - \epsilon \nabla _w \hat J(w^{(\tau -1)})\\ &= w^{(\tau -1)} - \epsilon H(w^{(\tau -1)}-w^*) \\ w^{(\tau)}-w^* &= (1-\epsilon H )(w^{(\tau -1)} - w^*)\end{aligned}$$

将$$H$$特征值分解为$$H = Q\Lambda Q^T$$,$$Q$$为特征向量的正交基底，可得

$$\begin{aligned}w^{(\tau)}-w^* &= (1-\epsilon Q\Lambda Q^T )(w^{(\tau -1)} - w^*)\\ Q^T(w^{(\tau)}-w^*) &=(I-\epsilon \Lambda)Q^T(w^{(\tau -1)}-w^*) \end{aligned}$$

假设$$w^{(0)} = 0$$,经过$$\tau$$次更新后，有$$Q^T w^{(\tau)} = \lvert I - (I - \epsilon \Lambda)^{\tau}\rvert Q^{T}w^*$$

$$\begin{aligned}Q^T w^{(\tau)} &= (\Lambda + \alpha I )^{-1} \Lambda Q^{T}w^* \\ Q^T w^{(\tau)} &= (I -(\Lambda + \alpha I )^{-1}\alpha)Q^{T}w^*\end{aligned}$$

> $$\alpha$$是学习速率，$$\tau$$是迭代次数，$$\epsilon$$是正则化系数，好多公式推导没太看懂

如果$$\epsilon$$, $$\alpha$$和$$\tau$$满足$$ (I - \epsilon \Lambda)^T = (\Lambda + \alpha I)^{-1}\alpha$$则L2正则化和提前终止可以看作等价的。进一步取对数，使用$$\log(1+x)$$的级数展开，可以得出结论: 如果$$\epsilon \lambda_i \ll 1, \lambda_i /\alpha \ll 1$$，则 $$ \tau \approx \dfrac{1}{\epsilon \alpha}$$, $$\alpha \approx \dfrac{1}{\tau \epsilon}$$

> 说明迭代次数$$\tau$$起着和L2参数反比的作用

