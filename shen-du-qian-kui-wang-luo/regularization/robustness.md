---
description: 噪声鲁棒性
---

# Robustness

### 用于输入和隐藏单元

对于某些模型而言，向输入添加方差极小的噪声等价于对权重施加范数惩罚。一般情况下，注入的噪声远比简单收缩参数强大，特别是噪声被添加到隐藏单元时。Dropout算法就是这种做法的主要发展方向。

### 用于权重

另一种正则化模型的噪声使用方式时将其添加到权重。这个技术主要用于循环神经网络。可以被解释为关于权重的贝叶斯推断的随机实现。

> 即贝叶斯学习过程将权重视为不确定，所以向权重添加噪声就是反映这种不确定性

某些假设下，施加于权重的噪声可以被解释为与更传统的正则化形式等同，鼓励要学习的函数保持稳定。我们研究回归的情形，也就是训练将一组特征$$x$$映射成一个标量的函数$$\hat y(x)$$并使用最小二乘代价函数衡量模型预测值与真实值的误差: $$J = \mathbb{E}_{p(x,y)}[(\hat y(x)-y)^2]$$。假设对每个输入表示，网络权重添加随机扰动$$\epsilon_w $$。想象我们有个$$l$$层MLP，我们将扰动记位$$\hat y_{\epsilon w}(x)$$。此时目标函数$$J = \mathbb{E}_{p(x,y,e,w)}[(\hat y_{ew}(x)-y)^2] = J\mathbb{E}_{p(x,y,e,w)}[(\hat y^2_{ew}(x)-2y\hat y_{ew}(x)+y^2]$$

如果噪声的方差$$\eta l$$足够小，则等同于添加正则化项$$\eta \mathbb{E}_{p(x,y)}[||\nabla_W \hat y(x)||^2]$$这种形式的正则化鼓励参数进入权重小扰动对输出相对影响较小的参数空间区域。换句话说，它推动模型进入对权重小的变化相对不敏感的区域，找到的点不只是极小点，而是有平坦区域包围的极小点。

> 没太看懂，是如何对应的？

### 用于输出

如果数据集上的标签有错误，会不利于模型的分布拟合现实分布。这种情况下，可以显示式地对标签上的噪声进行建模。我们可以假设对于一些小常数$$\epsilon$$，标签正确的概率是$$1-\epsilon$$。这个假设很容易能与代价函数结合。比如标签平滑（label smoothing）通过把确切分类目标从0和1替换成$$\dfrac{\epsilon}{k-1}$$和$$1-\epsilon$$，正则化具有k个输出的softmax函数模型。这样，最大似然学习就不会无止境地进行。

