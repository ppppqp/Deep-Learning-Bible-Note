# Regularization and Underconstrained Problem

很多机器学习的线性模型都依赖于矩阵$$X^TX$$求逆。只要$$X^TX$$是奇异的，这些方法都会失效。当数据生成分布在一些方向上确实没有差异时，或因为例子较少，而在一些方向上没有方差时，这个矩阵就时奇异的。正则化对于许多形式对应求逆$$X^TX + \alpha I$$。这个正则化矩阵可以保证是可逆的。

> 为什么正则化对应求$$X^TX + \alpha I$$的逆？
>
> 为什么在一些方向上没有观察到方差，表示矩阵奇异？在线性回归模型时，存在这样一种假设，即各个行向量之间不存在很强的关系。如果行向量之间存在很强的线性相关关系，就认为数据之间存在共线性问题。共线性会导致回归参数不稳定，即增加或删除一个样本点或特征，回归系数的估计值会发生很大变化。 这是因为某些解释变量之间存在高度相关的线性关系，XTX会接近于奇异矩阵，即使可以计算出其逆矩阵，逆矩阵对角线上的元素也会很大，这就意味着参数估计的标准误差较大，参数估计值的精度较低，这样，数据中的一个微小的变动都会导致回归系数的估计值发生很大变化。  
> 可以使用岭回归解决这个问题。
>
> [详情](https://zhuanlan.zhihu.com/p/72722146)

相关矩阵可逆时，这些线性问题有闭式解。没有闭式解的问题也可能是欠定的。一个例子是应用于线性可分问题的逻辑回归。如果权重向量w能够实现完美分类，那么2w也会以更高似然实现完美分类。类似的随机梯度下降的迭代优化算法将持续增加w的大小，理论上永远不会停止。在实践中，数值实现的梯度下降最终会达到导致数值溢出的最大权重。

> 2w将会把分类的点映射得更加分散，对应的正例就会更加接近1，负例更加接近-1，所以总体的loss会降低。



