# Regularization and Underconstrained Problem

很多机器学习的线性模型都依赖于矩阵$$X^TX$$求逆。只要$$X^TX$$是奇异的，这些方法都会失效。当数据生成分布在一些方向上确实没有差异时，或因为例子较少，而在一些方向上没有方差时，这个矩阵就时奇异的。正则化对于许多形式对应求逆$$X^TX + \alpha I$$。这个正则化矩阵可以保证是可逆的。

> 为什么正则化对应求$$X^TX + \alpha I$$的逆？
>
> 为什么在一些方向上没有观察到方差，表示矩阵奇异？因为此时矩阵中的某几列是固定值。也就是说，作差可能为零，导致

相关矩阵可逆时，这些线性问题有闭式解。没有闭式解的问题也可能是欠定的。一个例子是应用于线性可分问题的逻辑回归。如果权重向量w能够实现完美分类，那么2w也会以更高似然实现完美分类。类似的随机梯度下降的迭代优化算法将持续增加w的大小，理论上永远不会停止。在实践中，数值实现的梯度下降最终会达到导致数值溢出的最大权重。

> 2w将会把分类的点映射得更加分散，对应的正例就会更加接近1，负例更加接近-1，所以总体的loss会降低。



