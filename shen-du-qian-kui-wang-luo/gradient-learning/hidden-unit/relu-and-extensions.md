# ReLU and extensions

## ReLU（整流线性单元）

在原点处不可微分问题：不值一提，其实只需要取左逼近或者右逼近中一个就可以了，不用强求二者相等。

## 特性

* ✅    激活状态保持统一的，较大的梯度，避免了梯度爆炸和梯度消失
* ✅    计算速度快
* ❌    无法通过梯度的方法学习使得他们激活为零的样本，即不能对模型参数产生影响。

{% hint style="warning" %}
> 整流操作的二阶导数几乎处处为零，并且在整流线性单元处于激活状时，它的一阶导数处处为1。这意味着相比引入二阶效应的激活函数来说，它的梯度方向对学习来说更加有用。

导数为1且处处相同的好处是，在链式法则的时候，能不失真地，且与weight大小无关地传递梯度。

**如果导数不为1：**会导致梯度在传递的时候，增加一个指数项，导致梯度爆炸和梯度消失

**如果导数不处处相同：**会导致梯度传递的大小和weight的大小有关。比如是激活函数有不等的二阶导数是$$f^{\prime\prime}(x)=x$$，则对于不同的$$x$$, 传递的梯度会不同，导致传递时失真。如果有一层的值非常小，那么传递的时候就会让梯度变得很小，backprop后面的层就更难以感知到梯度变化了。
{% endhint %}

## 扩展

为了解决无法学习激活为零的样本，产生了几种变种。

### 绝对值整流：





