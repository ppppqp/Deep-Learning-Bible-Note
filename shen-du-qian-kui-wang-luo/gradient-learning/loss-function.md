# Loss Function

代价函数的选择是非常重要的。神经网络的代价函数和其他的参数模型大致相同。一般情况下，参数模型定义了一个分布$$p(y|x, \theta)$$, 并且使用最大似然原理。所以我们使用训练数据和模型预测间的交叉熵作为代价函数。

理解几个点：

* $$p(y|x,\theta)$$是给定$$x, \theta$$, 所求得的$$y$$的概率分布。
* **训练数据和模型预测：**我们假定训练数据和真实世界的分布一致，希望模型预测的概率尽可能接近训练数据。
* **交叉熵：**定义了两个分布之间的相似度。

{% hint style="danger" %}
我们所关心的随机变量，其实是一个条件概率，即$$p(y|x)$$，给定输入，输出的分布。我们要找出使得该条件概率出现可能性最大的一组参数。
{% endhint %}

### 1. 使用最大似然学习条件分布

代价函数就是负的对数似然，与训练数据和模型分布间的交叉熵等价。公式为

$$J(\theta) = -\mathbf{E}_{x,y ～ \hat p_{data}}\log{p_{model}(y|x)}$$

理解上是：

* $$J(\theta)$$关于一组参数的loss
* $$\mathbf{E}_{x,y～\hat p_{data}}$$:符合训练数据的随机变量的分布的数学期望
* $$p_{model}$$符合模型数据的随机变量的数学期望

以上就是交叉熵的公式。想知道为什么是这样？可以参考[交叉熵和最大似然估计简介](https://windmising.gitbook.io/mathematics-basic-for-ml/gai-shuai-lun/likelihood#jiao-cha-shang)。简单理解需要以下公式：

* **最大似然公式：**$$P(X|\theta) = P(x^{(1)}|\theta)P(x^{(2)}|\theta)...P(x^{(n)}|\theta)$$ ，简单来说就是对于这一系列参数，随机变量输入可能是这些值的概率。最大似然估计就是找$$\theta$$使得$$P(X|\theta)$$最大。
* **最大对数似然估计：** $$\log P(X|\theta) = \sum \log P(x^{i} | \theta)$$两边取对数，乘法变加法
* **代入期望：**  $$\sum \log P(x^{i} | \theta) = \sum p_{x,y ～ \hat p_{data}} \log p_{model}(y|x) = m \mathbf{E_{x,y ～\hat p_{data}}}\log p_{model}(y|x)$$理解成 概率\*值的和的形式，概率是真实的数据分布，值是模型给出的分布的对数

### 优点：

1. 减轻了为每个模型设计代价函数的负担。即对于任意模型$$p(y|x)$$, 代价函数就是$$\log p(y|x)$$
2. 由于使用了对数，使得消除了指数函数的影响，不容易饱和

### 特性：

* 没有最小值

### 实操：使用交叉熵

假设我们有一个二分类问题，标签为0, 1。输出使用的是sigmoid，输出是该样本标签为1的概率。

* $$p_{model}({y=1|x} )= \sigma(z)$$ 
* $$p_{model}({y=0|x} )= 1 - \sigma(z)$$
* 整合上两式，用一个统一的表达式：$$p_{model}(y|x) =\sigma(z)^{y}(1-\sigma(z)^{1-y})$$
* 代入数学期望，有$$ J(x) = -y\log\sigma(z) -(1-y)(1-\log\sigma(z))$$ 理解就是，前一项是y=1时，后一项是y=0时

### 2. 学习条件统计量

{% hint style="info" %}
学习$$p(f(y) | x ;\theta)$$而不是$$p(y|\theta)$$
{% endhint %}

一个预测器$$f(x;\theta)$$表示以$$x$$为输入，$$\theta$$为参数。想用它来预测$$y$$的均值。那可以把这个神经网络的各部分理解为一个范函。

#### 泛函：

{% hint style="info" %}
范函是一个函数到数的映射
{% endhint %}

如果我们通过希望找到其中满足特殊条件的函数，可以通过设计代价泛函来得到。对函数的求解优化需要用到变分法，原理上和求导等于零类似，把优化问题转化成微分方程的形式。

例如，如果我们希望是的神经网络优化的最终结果是输入能给出给定$$x$$, $$y$$的期望值（回归问题），我们可以通过变分法，将优化问题$$f^{*} = \arg_{f}\min\mathbf{E}_{x,y～p_{data}}|| y-f(x)||^{2}$$求得$$ f^{*}(x)=\mathbf{E}_{y～p_{data}(y|x)}[y]$$。所以，如果我们以这个函数作为loss function，则可以得到我们想要的功能。

