# Loss Function

代价函数的选择是非常重要的。神经网络的代价函数和其他的参数模型大致相同。一般情况下，参数模型定义了一个分布$$p(y|x, \theta)$$, 并且使用最大似然原理。所以我们使用训练数据和模型预测间的交叉熵作为代价函数。

理解几个点：

* $$p(y|x,\theta)$$是给定$$x, \theta$$, 所求得的$$y$$的概率分布。
* **训练数据和模型预测：**我们假定训练数据和真实世界的分布一致，希望模型预测的概率尽可能接近训练数据。
* **交叉熵：**定义了两个分布之间的相似度。

{% hint style="danger" %}
我们所关心的随机变量，其实是一个条件概率，即$$p(y|x)$$，给定输入，输出的分布。我们要找出使得该条件概率出现可能性最大的一组参数。
{% endhint %}

### 1. 使用最大似然学习条件分布

代价函数就是负的对数似然，与训练数据和模型分布间的交叉熵等价。公式为

$$J(\theta) = -\mathbf{E}_{x,y ～ \hat p_{data}}\log{p_{model}(y|x)}$$

理解上是：

* $$J(\theta)$$关于一组参数的loss
* $$\mathbf{E}_{x,y～\hat p_{data}}$$:符合训练数据的随机变量的分布的数学期望
* $$p_{model}$$符合模型数据的随机变量的数学期望

以上就是交叉熵的公式。想知道为什么是这样？可以参考[交叉熵和最大似然估计简介](https://windmising.gitbook.io/mathematics-basic-for-ml/gai-shuai-lun/likelihood#jiao-cha-shang)。简单理解需要以下公式：

* **最大似然公式：**$$P(X|\theta) = P(x^{(1)}|\theta)P(x^{(2)}|\theta)...P(x^{(n)}|\theta)$$ ，简单来说就是对于这一系列参数，随机变量输入可能是这些值的概率。最大似然估计就是找$$\theta$$使得$$P(X|\theta)$$最大。
* **最大对数似然估计：** $$\log P(X|\theta) = \sum \log P(x^{i} | \theta)$$两边取对数，乘法变加法
* **代入期望：**  $$\sum \log P(x^{i} | \theta) = \sum p_{x,y ～ \hat p_{data}} \log p_{model}(y|x) = m \mathbf{E_{x,y ～\hat p_{data}}}\log p_{model}(y|x)$$理解成 概率\*值的和的形式，概率是真实的数据分布，值是模型给出的分布的对数



### 优点：

1. 减轻了为每个模型设计代价函数的负担。即对于任意模型$$p(y|x)$$, 代价函数就是$$\log p(y|x)$$
2. 由于使用了对数，使得消除了指数函数的影响，不容易饱和

### 特性：

* 没有最小值



### 实操：使用交叉熵

假设我们有一个二分类问题，标签为0, 1。输出使用的是sigmoid，输出是该样本标签为1的概率。

* $$p_{model}({y=1|x} )= \sigma(z)$$ 
* $$p_{model}({y=0|x} )= 1 - \sigma(z)$$
* 整合上两式，用一个统一的表达式：$$p_{model}(y|x) =\sigma(z)^{y}(1-\sigma(z)^{1-y})$$
* 代入数学期望，有$$ J(x) = -y\log\sigma(z) -(1-y)(1-\log\sigma(z))$$ 理解就是，前一项是y=1时，后一项是y=0时



